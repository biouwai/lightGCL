## 一、超参数

1. batch_user=256 ：
   - 这个参数在测试阶段用于分批次处理用户（这里可能指非编码 RNA 或药物），每次处理 256 个。分批次可以减少内存使用，避免一次性加载所有数据，同时可能影响模型的预测稳定性和速度。需要确认代码中是否在测试时使用了这个参数，比如在生成预测时如何分批处理用户。
     d=64 ：嵌入维度，决定用户和物品（非编码 RNA 和药物）的向量表示长度。维度越高，模型的表达能力越强，但可能导致过拟合。需要结合数据量和特征复杂度来选择合适的维度。
2. dropout=0.2 ：
   - 在训练过程中随机丢弃 20%的神经元，防止过拟合。在图卷积层中使用，可能影响模型的泛化能力，尤其是在数据量较小的情况下。
3. epoch_no=120 ：
   - 训练的总轮数。需要足够多的 epoch 让模型收敛，但过多可能导致过拟合。用户可能需要根据验证集的表现调整这个参数。
4. l=2 ：
   - 图卷积的层数。层数越多，模型能捕捉到的高阶交互越复杂，但也会增加计算负担和过拟合风险。在生物关联预测中，可能需要平衡层数和模型性能。
5. lambda_1=0.005 和 lambda_2=0.00001 ：
   - 这两个参数分别控制对比损失和正则化损失的权重。lambda_1 调整对比学习对总损失的贡献，影响模型学习特征的区分能力；lambda_2 控制 L2 正则化的强度，防止参数过大。
6. lr=0.001 ：
   - 学习率，决定参数更新的步长。较高的学习率可能加快收敛，但可能导致震荡；较低的学习率可能更稳定但收敛慢。需要结合优化器（如 Adam）的默认参数来调整。
7. svd_q=64 ：
   - 在矩阵分解中保留的奇异值数量，影响 SVD 初始化的嵌入质量。较大的 q 可能保留更多原始信息，但增加计算量。在生物数据中，可能需要根据矩阵的秩来选择合适的 q 值。
8. temp=0.1 ：
   - 对比损失中的温度参数，控制相似性和不相似性样本之间的分布。较低的温度值会使模型更关注相似样本，可能提高区分度，但过低可能导致梯度消失。
   - 当 Temperature 值较高时，softmax 函数的输出会更加“平滑”，即各个类别的概率分布会更加均匀，模型更倾向于给出多样化的输出。相反，当 Temperature 值较低时，softmax 函数的输出会更加“尖锐”，即某个类别的概率会显著高于其他类别，模型更倾向于给出确定的输出。
   - 当模型的「温度」较高时（如 0.8、1 或更高），模型会更倾向于从较多样且不同的词汇中选择，这使得生成的文本风险性更高、创意性更强，但也可能产生更多的错误和不连贯之处。而当「温度」较低时（如 0.2、0.3 等），模型主要会从具有较高概率的词汇中选择，从而产生更平稳、更连贯的文本。但此时，生成的文本可能会显得过于保守和重复。因此在实际应用中，需要根据具体需求来权衡选择合适的「温度」值。总结：管理温度是一种微妙的平衡行为。设置得太高，模型可能会产生无意义的或不相关的反应。设置得太低，模型的输出可能会显得过于机械化或缺乏多样性。因此，温度参数在将人工智能的性能微调到最佳水平方面起着关键的作用。
   - 低温度（如 temp=0.1） ：
     适合生物数据的稀疏性和噪声场景，通过放大正样本相似性差异：
     增强模型对已知关联（正样本）的置信度。
     抑制噪声负样本的影响（如随机关联的低相似性被进一步降低权重）。
     高温度（如 temp=0.5） ：
     可能更适合关联模式较密集的场景，但需警惕过拟合假阳性关联。

## 二、数据处理函数

1. COO 矩阵
   COO 格式通过三个数组存储稀疏矩阵：
   data: 非零元素的值。
   row: 非零元素所在的行索引。
   col: 非零元素所在的列索引。
   例如，一个稀疏矩阵：

   ```py
   [[1, 0, 0],
   [0, 2, 0],
   [0, 0, 3]]
   ```

   在 COO 格式中可以表示为：

   ```py
   data = [1, 2, 3]
   row = [0, 1, 2]
   col = [0, 1, 2]
   ```

   优点：
   简单直观：直接记录非零元素的位置和值。
   易于构建：适合从文件或数据流中快速生成稀疏矩阵。

​

## 三、归一化

归一化的本质是为了平衡数据中热门项和冷门项的影响 ，从而让模型能够更加全面地学习数据中的模式，而不是过度偏向于热门项。这在 RNA 和药物耐药性预测任务中尤为重要，因为如果不加以处理，模型可能会忽略冷门 RNA 或药物的信息，导致预测结果不够全面或公平。

## TrnData

1. self.dokmat :
   将稀疏矩阵从 COO 格式转换为 DOK（Dictionary of Keys）格式。
   DOK 格式是一种基于字典的稀疏矩阵表示方式，支持快速查找某个位置是否存在非零值。
   例如，(u, i) in self.dokmat 可以快速判断用户 u 是否与物品 i 存在交互。

2. train_loader = data.DataLoader(train_data) 的作用是将数据集 train_data 包装成一个可迭代的数据加载器，方便我们在训练模型时按批次加载数据。它支持多种功能（如批量加载、数据打乱、多线程加载等

## 四、SVD

1. adj_norm = scipy_sparse_mat_to_torch_sparse_tensor(train) ==> 其实就是转换一个格式，都是描述矩阵
2. .coalesce() 的作用是合并重复的索引，并确保稀疏张量的格式是规范化的。
3. adj_dense = adj.to_dense()
   将稀疏张量 adj 转换为密集张量（普通的二维数组）。
   密集张量更适合在后续的 SVD 分解中使用。
4. 矩阵的低秩近似表示：
   u_mul_s = svd_u @ (torch.diag(s))
   v_mul_s = svd_v @ (torch.diag(s))
   左奇异向量 U ：
   用于提取与矩阵行相关的特征。
   例如，在用户-物品交互矩阵中，U 提取的是用户的特征。
   右奇异向量 V ：
   用于提取与矩阵列相关的特征。
   例如，在用户-物品交互矩阵中，V 提取的是物品的特征。
   U 的每一行表示一个用户的低维表示（嵌入）。
   V 的每一列表示一个物品的低维表示（嵌入）。
   例如，V[j] 是第 j 个物品的特征向量，描述了该物品的特性。

## 五、模型初始化

```py
# 假设五个用户，嵌入维度为3
self.E*u_0 = nn.Parameter(nn.init.xavier_uniform*(torch.empty(n*u, d)))
self.E_i_0 = nn.Parameter(nn.init.xavier_uniform*(torch.empty(n_i, d)))
用户嵌入矩阵 E_u_0:
Parameter containing:
tensor([[0.3162, -0.4975,  0.5803],
        [-0.4369,  0.3628, -0.5515],
        [ 0.5532,  0.4719,  0.3268],
        [-0.3698, -0.5634,  0.4183],
        [ 0.4943, -0.3368, -0.5883]], requires_grad=True)

物品嵌入矩阵 E_i_0:
Parameter containing:
tensor([[0.4123, -0.5612,  0.3845],
        [-0.3265,  0.4729, -0.4983],
        [ 0.5231,  0.3856,  0.4192],
        [-0.4867, -0.5123,  0.3689]], requires_grad=True)
```

1. self.Z_u_list
   邻接矩阵传播
   稀疏矩阵乘法 (torch.spmm)
   捕获用户与邻居物品之间的局部关系
2. self.G_u_list
   低秩分解更新
   低秩分解矩阵运算
   捕获用户与物品之间的全局结构信息
3. self.E_u_list
   综合 Z_u_list 和 G_u_list
   聚合操作（如直接赋值或加权和）
   用户节点在当前层的最终嵌入表示

## 六、forword 函数 -- 邻接矩阵传播

self.Z_u_list[layer] = (torch.spmm(sparse_dropout(self.adj_norm, self.dropout), self.E_i_list[layer - 1]))
self.Z_i_list[layer] = (torch.spmm(sparse_dropout(self.adj_norm, self.dropout).transpose(0, 1), self.E_u_list[layer - 1]))

1. 它们通过稀疏矩阵乘法（torch.spmm）实现了用户和物品之间的信息交换。
2. 用户节点表示更新 ：
   通过邻接矩阵将物品嵌入信息传播到用户节点。
   捕获用户与其邻居物品之间的局部关系。
   更新后的结果存储在 self.Z_u_list[layer] 中。
3. 物品节点表示更新 ：
   通过转置后的邻接矩阵将用户嵌入信息传播到物品节点。
   捕获物品与其邻居用户之间的局部关系。
   更新后的结果存储在 self.Z_i_list[layer] 中。
4. torch.spmm 是稀疏矩阵乘法操作
5. .transpose(0, 1)
   将邻接矩阵转置，表示从用户节点向物品节点传播信息。
   转置后的邻接矩阵描述了物品与用户之间的连接关系。
6. 物品节点通过转置后的邻接矩阵从其邻居用户节点接收信息。
   每个物品的嵌入表示是其邻居用户嵌入的加权和。

## 七、SVD 传播

1. 用户节点的低秩分解更新 ：
   将物品嵌入投影到低维空间（vt_ei = self.vt @ self.E_i_list[layer - 1]）。
   再从低维空间恢复到用户嵌入空间（self.G_u_list[layer] = self.u_mul_s @ vt_ei）。
   捕获用户和物品之间的全局结构信息。
2. 邻接矩阵传播：
   捕获用户和物品之间的局部关系，基于图结构中的直接连接（邻居节点）进行信息传播。
   低秩分解更新：
   捕获用户和物品之间的全局结构信息，通过低秩分解矩阵提取高维交互矩阵中的潜在模式，忽略具体的图结构。

## 八、跨层聚合

跨层聚合的主要目的是充分利用 GNN 各层的信息，避免信息丢失或过度依赖某一层的嵌入表示。以下是几个关键原因：

(1) 综合多层次信息
浅层嵌入通常捕获局部的、低阶的关系（如直接邻居之间的连接）。
深层嵌入通常捕获全局的、高阶的关系（如间接邻居或更复杂的结构信息）。
通过跨层聚合，可以同时利用局部和全局信息，提升模型的表达能力。

## 九、对比学习

这两行代码的核心作用是对用户嵌入向量进行 L2 归一化，使其具有单位长度。这种操作在对比学习中非常重要，因为它能够使嵌入向量的点积直接反映它们的方向相似性，从而提高模型的效果。

## 十、BPR 损失

BPR 损失是一种基于排序的损失函数，主要用于推荐系统中的隐式反馈数据（如点击、购买等）。它通过最大化用户对正样本（用户交互过的物品）的偏好与负样本（用户未交互过的物品）之间的差异来训练模型。
目标 : 让正样本的预测得分高于负样本。
应用场景 : 主要用于排序任务，强调区分用户喜欢和不喜欢的物品。
局限性 : 仅关注正负样本之间的相对关系，可能无法捕捉全局的嵌入分布特性。

目标 : 学习到更鲁棒和泛化的嵌入表示，使正样本嵌入之间的相似性更高，负样本嵌入之间的相似性更低。
应用场景 : 常用于自监督学习任务，帮助模型从数据中提取更多的信息。

## 十一、正则化损失

正则化损失的核心目的是通过限制模型参数的大小来防止过拟合。

## 十二、优化器

这行代码的作用是创建一个 Adam 优化器，用于在训练过程中更新 LightGCL 模型的参数。通过自适应调整学习率，Adam 能够有效地优化模型，使其逐步逼近最优解。
